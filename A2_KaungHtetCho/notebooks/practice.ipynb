{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Experiment: artifact_location='mlflow-artifacts:/349721501222343077', creation_time=1738684060805, experiment_id='349721501222343077', last_update_time=1738684060805, lifecycle_stage='active', name='st124092-A2Assignment', tags={}>"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import mlflow\n",
    "import os\n",
    "\n",
    "\n",
    "mlflow.set_tracking_uri('http://localhost:8080')\n",
    "\n",
    "os.environ['LOGNAME'] = 'st124092'\n",
    "\n",
    "mlflow.set_experiment('st124092-A2Assignment')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_diabetes\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import numpy as np\n",
    "from time import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features: ['age', 'sex', 'bmi', 'bp', 's1', 's2', 's3', 's4', 's5', 's6']\n"
     ]
    }
   ],
   "source": [
    "diabetes = load_diabetes()\n",
    "print(f'Features: {diabetes.feature_names}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "features: (442, 10)\n",
      "target: (442,)\n"
     ]
    }
   ],
   "source": [
    "X = diabetes.data   \n",
    "y = diabetes.target\n",
    "\n",
    "print(f'features: {X.shape}')\n",
    "print(f'target: {y.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "442\n",
      "10\n"
     ]
    }
   ],
   "source": [
    "m = X.shape[0] # samples\n",
    "n = X.shape[1] # features\n",
    "\n",
    "print(m)\n",
    "print(n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train: (309, 10)\n",
      "X_test: (133, 10)\n"
     ]
    }
   ],
   "source": [
    "scaler  = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test  = scaler.transform(X_test)\n",
    "\n",
    "print(f'X_train: {X_train.shape}')\n",
    "print(f'X_test: {X_test.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.],\n",
       "       [1.],\n",
       "       [1.]])"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.ones((3,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "intercept: (309, 1)\n",
      "X_train: (309, 11)\n",
      "intercept: (133, 1)\n",
      "X_test: (133, 11)\n"
     ]
    }
   ],
   "source": [
    "intercept = np.ones((X_train.shape[0], 1))\n",
    "print(f'intercept: {intercept.shape}')\n",
    "\n",
    "X_train = np.concatenate((intercept, X_train), axis = 1)\n",
    "print(f'X_train: {X_train.shape}')\n",
    "\n",
    "intercept = np.ones((X_test.shape[0], 1))\n",
    "print(f'intercept: {intercept.shape}')\n",
    "\n",
    "X_test = np.concatenate((intercept, X_test), axis = 1)\n",
    "print(f'X_test: {X_test.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KFold(n_splits=5, random_state=None, shuffle=False)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "\n",
    "kfold = KFold(n_splits=5)\n",
    "\n",
    "print(kfold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (4185849342.py, line 24)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[71], line 24\u001b[0;36m\u001b[0m\n\u001b[0;31m    for\u001b[0m\n\u001b[0m        ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "\n",
    "class LinarRegression(object):\n",
    "\n",
    "    kfold = KFold(n_splits=5)\n",
    "\n",
    "    def __init__(self, regularization, lr=0.001, method='batch', num_epoches=500, batch_size=50, cv=kfold):\n",
    "        self.lr             = lr\n",
    "        self.num_epoches    = num_epoches\n",
    "        self.batch_size     = batch_size\n",
    "        self.method         = method\n",
    "        self.cv             = cv\n",
    "        self.regularization = regularization\n",
    "\n",
    "    def mse(self, ytrue, ypred):\n",
    "        return ((ypred - ytrue) ** 2).sum() / ytrue.shape[0]\n",
    "    \n",
    "    def fit(self, X_train, y_train):\n",
    "\n",
    "        self.kfold_scores = list()\n",
    "\n",
    "        self.val_loss_old = np.infty\n",
    "\n",
    "        for \n",
    "\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_cross_train: (247, 11)\n",
      "y_cross_train: [262. 257.  65. 202.  68. 150. 111. 292. 181.  99.  44.  91. 147. 168.\n",
      "  51. 131.  89. 101.  81.  40. 248. 182. 235. 292. 109. 118. 178.  65.\n",
      "  90.  83. 283. 217.  60. 220.  99. 187.  84. 252. 174. 121. 148.  64.\n",
      " 124. 220. 163. 221. 137.  96. 233. 198. 341. 122. 131. 178. 197.  87.\n",
      " 127.  75. 219. 172. 136. 125.  69. 202. 310. 100. 173. 104. 243.  78.\n",
      " 134.  48.  37. 199. 103.  72.  85. 120. 115.  90. 185. 182. 161. 138.\n",
      "  64. 158. 142.  61.  96. 274. 150. 236. 296.  65.  63. 111.  70. 264.\n",
      " 275.  98.  83. 245.  77.  70. 110.  84. 118. 168. 265. 217.  90. 310.\n",
      " 101.  67. 192. 281. 258. 144. 152. 283. 110.  78. 321. 115. 185. 295.\n",
      " 281.  59. 230. 151. 288. 146.  59. 141.  48. 249. 242. 144.  42. 139.\n",
      " 141. 242. 142. 210. 190. 144. 182. 265. 214. 262. 180. 229. 303. 252.\n",
      " 241.  94. 200. 166.  58. 243. 230.  39.  53.  53.  55. 332. 138. 263.\n",
      " 259. 270. 190.  91.  43.  79. 277. 140.  68.  81.  84.  91. 215. 174.\n",
      "  42.  45. 124. 135.  72. 198. 160. 170.  48. 214.  59. 280. 272. 259.\n",
      "  93. 102. 275. 173.  31. 121. 181.  85. 206. 151. 179.  61.  77. 196.\n",
      " 128. 237. 242.  85. 154.  53. 152. 114.  71.  63.  66. 206. 118.  49.\n",
      " 178. 216.  85.  88. 104. 245. 145. 336. 156. 170.  75.  90. 225.  47.\n",
      "  78. 201. 186. 111. 257.  73. 167.  95. 259.]\n",
      "X_cross_val: [[ 1.         -0.97927446 -0.94029056 -1.56714983 -1.7882835   1.6660266\n",
      "   0.54123468  3.85066353 -0.85658462 -0.1292791   0.06872652]\n",
      " [ 1.          1.52653775 -0.94029056  0.31648509  1.33406356  1.22411257\n",
      "   0.8415112   0.22502185 -0.0822227   1.15417099 -1.1953505 ]\n",
      " [ 1.         -1.8145452  -0.94029056  0.36080591 -0.59881795  0.09170789\n",
      "   0.37858489 -0.0166876  -0.0822227  -0.52972305  0.06872652]\n",
      " [ 1.          0.53939961 -0.94029056  1.84555343 -1.169018    0.14694714\n",
      "   0.77895359 -0.58067631  0.41336893 -1.0391569   0.40581372]\n",
      " [ 1.         -0.29587113 -0.94029056 -1.41202695 -1.54072598  0.00884901\n",
      "   0.32228304 -0.0166876  -0.17514613 -0.67610777 -1.0268069 ]\n",
      " [ 1.          0.76720072  1.06350105  0.27216427  0.4419644   0.78219855\n",
      "   0.88530153 -0.90295557  1.46650114  0.98373333  0.32154192]\n",
      " [ 1.         -1.28300927 -0.94029056  0.84833495 -0.45013476 -0.21210801\n",
      "  -0.16566632  0.54730111 -0.85658462 -0.60150606 -0.18408889]\n",
      " [ 1.          0.61533331  1.06350105  1.1585807   1.18538037 -0.46068464\n",
      "   0.00949499 -0.66124612 -0.0822227  -0.55340017 -0.35263249]\n",
      " [ 1.         -0.29587113  1.06350105 -1.21258325 -0.07842678 -0.57116315\n",
      "  -0.59731383  0.46673129 -0.85658462 -0.3938615   0.91144453]\n",
      " [ 1.         -0.37180483 -0.94029056  1.04777865  1.48274676 -0.68164166\n",
      "  -0.57229078  0.22502185 -0.85658462 -0.60150606 -0.18408889]\n",
      " [ 1.         -0.90334076  1.06350105  0.22784345 -0.15276837 -0.26734726\n",
      "   0.10958717 -1.54751409  1.46650114  0.68870893 -0.26836069]\n",
      " [ 1.         -1.13114187  1.06350105 -0.43696888  1.70577155  1.7765051\n",
      "   2.13645373 -0.82238575  2.24086306  0.70111123 -0.85826329]\n",
      " [ 1.         -0.97927446  1.06350105  2.75413027 -0.37579316  0.80981818\n",
      "   0.62255957 -0.90295557  1.46650114  1.42176     0.99571633]\n",
      " [ 1.          0.76720072  1.06350105  0.11704139  0.88801398 -0.87497904\n",
      "  -0.43466404 -0.82238575 -0.0822227  -0.41565948  0.15299832]\n",
      " [ 1.         -0.90334076 -0.94029056  0.78185372 -0.59881795 -0.65402203\n",
      "  -0.62233687 -0.74181594  0.03393159  0.65150203 -0.35263249]\n",
      " [ 1.         -1.20707557 -0.94029056 -1.7222727  -1.8626251  -0.76450054\n",
      "  -0.75370785  0.78901055 -0.85658462 -1.19982305 -2.79651472]\n",
      " [ 1.          0.76720072  1.06350105 -0.34832723 -0.37579316 -0.73688091\n",
      "  -0.45343132 -0.25839705 -0.0822227  -0.5772652  -0.01554528]\n",
      " [ 1.          0.3115985  -0.94029056  0.36080591  0.4419644  -0.29496688\n",
      "  -0.27827002 -0.09725741 -0.0822227   0.23226672  0.82717273]\n",
      " [ 1.         -1.28300927 -0.94029056 -0.03808148 -0.67315955 -0.15686875\n",
      "  -0.44717556  0.30559166 -0.0822227   0.61354348 -1.1110787 ]\n",
      " [ 1.          1.07093553 -0.94029056 -0.50345011 -1.63960031 -0.81973979\n",
      "  -0.76621937 -0.09725741 -0.85658462 -0.05486531 -1.1110787 ]\n",
      " [ 1.         -0.82740705 -0.94029056 -1.19042284 -1.7139419  -0.68164166\n",
      "  -1.74211809  3.1255352  -1.63094654 -0.4380212  -0.09981708]\n",
      " [ 1.         -0.14400372  1.06350105 -0.74721463 -0.37579316  0.14694714\n",
      "   0.30977152  0.38616148 -0.0822227  -0.62612274 -1.5324377 ]\n",
      " [ 1.          0.69126701  1.06350105 -0.68073339  0.07025642  1.85936398\n",
      "   1.7235735   0.78901055 -0.0822227   0.45362898 -0.77399149]\n",
      " [ 1.         -0.59960594  1.06350105 -0.54777093 -0.37579316  0.25742565\n",
      "   0.24095815  0.62787092 -0.0822227  -0.2670198  -0.43690429]\n",
      " [ 1.         -0.59960594 -0.94029056  1.75691179 -0.57428523  0.4231434\n",
      "   0.83525544 -0.66124612  0.69213922  0.01672979  1.58561894]\n",
      " [ 1.          1.83027256 -0.94029056  0.07272057  0.516306    0.58886116\n",
      "   0.15963325  1.43356908 -0.85658462  0.15221551  0.65862912]\n",
      " [ 1.         -1.8904789  -0.94029056 -0.92449791 -0.45013476 -1.34451269\n",
      "  -1.49814341  0.22502185 -0.85658462 -0.01859798 -0.60544789]\n",
      " [ 1.         -1.05520816 -0.94029056 -1.23474366 -0.97052594 -0.93021829\n",
      "  -0.87882307  0.06388222 -0.85658462 -0.28750238  0.32154192]\n",
      " [ 1.         -0.14400372 -0.94029056  0.82617454  1.85445474  0.25742565\n",
      "   0.12835445 -0.25839705 -0.0822227   0.83058372 -0.35263249]\n",
      " [ 1.          0.99500183 -0.94029056  1.18074111 -0.52447636 -0.48830427\n",
      "  -1.46686461 -0.90295557 -0.0822227   2.10445022  0.74290093]\n",
      " [ 1.          0.3115985   1.06350105  0.22784345 -0.00408518  0.00884901\n",
      "  -0.4409198  -0.66124612  0.69213922  1.5031266   0.82717273]\n",
      " [ 1.         -0.67553965  1.06350105  0.51592879  1.48274676 -2.55977626\n",
      "  -2.14874256 -1.14466501 -1.03468786 -0.67610777  0.32154192]\n",
      " [ 1.          0.15973109  1.06350105 -0.17104395 -1.41657552  1.38983033\n",
      "   1.66101589 -0.82238575  1.46650114  0.77289423  0.40581372]\n",
      " [ 1.          1.98213997 -0.94029056  0.75969331 -1.26789232  0.89267705\n",
      "   1.19808958 -0.41953668  0.73085731  0.21648198  0.49008552]\n",
      " [ 1.          1.29873664  1.06350105 -0.14888354 -0.30145157  2.05270137\n",
      "   0.96037066  1.27242944 -0.0822227   1.69949634 -0.35263249]\n",
      " [ 1.          0.84313442 -0.94029056 -0.99097915  0.71479806  0.86505743\n",
      "  -0.3283161   0.86958037 -0.33001852  1.81092913  0.15299832]\n",
      " [ 1.          0.3115985  -0.94029056  0.47160797  1.23518924 -1.23403419\n",
      "  -0.89759036 -0.25839705 -0.74043033 -1.10192005 -0.52117609]\n",
      " [ 1.          0.53939961  1.06350105 -0.79153545  0.66498919 -0.23972763\n",
      "   0.01575075 -0.17782723 -0.0822227  -0.35064136 -1.0268069 ]\n",
      " [ 1.          0.4634659  -0.94029056  1.20290152  1.11103877 -0.70926128\n",
      "  -0.99142677 -0.58067631 -0.0822227   1.08577043 -0.09981708]\n",
      " [ 1.          0.15973109 -0.94029056  0.27216427  0.07025642  0.17456677\n",
      "   0.19716782 -0.58067631  0.69213922  0.76105568  2.42833694]\n",
      " [ 1.          0.15973109  1.06350105  0.56024961  0.88801398 -0.07400987\n",
      "   0.72265174 -1.38637446  1.46650114 -0.52972305  0.32154192]\n",
      " [ 1.          0.3115985  -0.94029056  0.29432468 -0.52447636  1.19649295\n",
      "   0.87279001  0.70844074 -0.0822227   0.72535209 -0.01554528]\n",
      " [ 1.          0.61533331  1.06350105  1.09209947  1.63142995  0.97553593\n",
      "  -0.25950273 -0.74181594  1.46650114  2.43254741  1.83843434]\n",
      " [ 1.         -1.13114187 -0.94029056 -1.21258325 -0.82184274  1.7765051\n",
      "  -0.07808566  0.22502185  0.69213922  2.68660664  0.06872652]\n",
      " [ 1.          0.3875322   1.06350105  0.1392018   0.3676228   0.28504527\n",
      "   0.87904577 -1.30580464  1.46650114  0.15221551 -1.2796223 ]\n",
      " [ 1.          0.53939961  1.06350105  0.29432468 -0.74750115 -0.15686875\n",
      "   0.90406881 -1.38637446  1.46650114 -2.03021339 -1.1953505 ]\n",
      " [ 1.         -1.51081038 -0.94029056  1.84555343  0.24347234  0.39552378\n",
      "   0.83525544  0.06388222 -0.02027375 -1.16656234 -0.01554528]\n",
      " [ 1.         -1.13114187 -0.94029056 -0.0824023  -0.59881795  0.20218639\n",
      "   0.12835445  1.11128981 -0.85658462 -0.78058775  0.40581372]\n",
      " [ 1.         -0.90334076  1.06350105  0.02839975 -1.19355073 -0.90259867\n",
      "  -0.55977926 -1.70865372  1.46650114  0.7131377   0.40581372]\n",
      " [ 1.          0.3875322  -0.94029056 -1.25690407 -1.26789232 -0.04639025\n",
      "  -0.26575849  1.67527852 -0.85658462 -1.30336347 -1.5324377 ]\n",
      " [ 1.         -1.9664126  -0.94029056 -0.9023375  -0.45013476 -1.39975195\n",
      "  -1.59823559  0.95015018 -1.63094654 -0.89108096 -1.95379671]\n",
      " [ 1.         -1.9664126  -0.94029056  0.51592879 -0.37579316  0.7269593\n",
      "   0.48493283  1.27242944 -0.85658462 -0.14825838 -0.01554528]\n",
      " [ 1.          0.4634659   1.06350105 -0.83585627 -1.04486753 -1.84166597\n",
      "  -1.52316646 -0.66124612 -0.85658462 -0.67610777 -0.26836069]\n",
      " [ 1.          1.45060405  1.06350105 -0.70289381  0.4419644  -0.76450054\n",
      "  -0.9538922   0.78901055 -0.85658462 -0.35064136 -0.01554528]\n",
      " [ 1.          0.91906812  1.06350105  0.3386455   0.81367238  0.39552378\n",
      "   0.50370011  0.22502185 -0.0822227  -0.11029983  0.40581372]\n",
      " [ 1.         -2.19421371  1.06350105 -0.54777093 -0.52447636 -1.75880709\n",
      "  -1.36677243 -0.33896686 -0.85658462 -1.65307073 -1.4481659 ]\n",
      " [ 1.          0.3115985  -0.94029056  1.44666604 -0.92071707 -0.1016295\n",
      "  -0.28452578 -0.25839705 -0.05124822  0.84204645  0.91144453]\n",
      " [ 1.         -1.51081038  1.06350105 -1.61147065 -0.89618434 -1.0406968\n",
      "  -0.31580458 -0.82238575 -0.0822227  -2.03021339 -0.68971969]\n",
      " [ 1.         -2.19421371 -0.94029056 -0.83585627 -0.59881795  0.03646863\n",
      "   0.38484065  0.30559166 -0.0822227  -1.45087566 -0.52117609]\n",
      " [ 1.          0.3875322   1.06350105  0.22784345  1.33406356  0.28504527\n",
      "   0.39109641 -0.98352538  0.69213922  0.91965478  1.83843434]\n",
      " [ 1.         -0.82740705  1.06350105  1.40234522 -1.26789232  3.07462755\n",
      "   3.09984093  0.06388222  1.48198838  0.99425649  1.41707533]\n",
      " [ 1.          0.76720072  1.06350105  0.05056016  0.66498919  0.11932751\n",
      "   0.33479456 -0.50010649  0.69213922  0.26327247  1.24853173]]\n",
      "y_cross_val: [ 57. 248. 246. 140.  96. 212. 129. 263. 135. 293. 142. 171. 233. 127.\n",
      "  52.  72.  83. 281. 258. 131.  60.  54.  69.  71. 279. 109. 185.  68.\n",
      " 195. 132. 220. 170. 116. 180. 202.  90.  52.  53. 281. 268. 244. 128.\n",
      " 310. 109. 235.  69. 200. 113. 144.  63.  97. 302. 129.  66. 237.  71.\n",
      " 275.  55. 113. 297. 220. 311.]\n"
     ]
    }
   ],
   "source": [
    "cv = kfold\n",
    "''' \n",
    "  Generate indices to split data into training and test set\n",
    "  k = training --> 247\n",
    "  v = test --> 62\n",
    "  total samples = 309 / 5 = 62 \n",
    "''' \n",
    "for i, (k,v) in enumerate(cv.split(X_train)): \n",
    "\n",
    "    # print(f'i: {i}')\n",
    "    # print(f'k: {len(k)}')\n",
    "    # print(f'k: {k}')\n",
    "\n",
    "    # print(f'v: {len(v)}')    \n",
    "    # print(f'v: {v}')    \n",
    "\n",
    "    X_cross_train = X_train[k]\n",
    "    print(f'X_cross_train: {X_cross_train.shape}')\n",
    "    y_cross_train = y_train[k]\n",
    "    print(f'y_cross_train: {y_cross_train}')\n",
    "\n",
    "    X_cross_val = X_train[v]\n",
    "    print(f'X_cross_val: {X_cross_val}')\n",
    "    y_cross_val = y_train[v]\n",
    "    print(f'y_cross_val: {y_cross_val}')\n",
    "\n",
    "    break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(11,)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "theta = np.zeros(X_cross_train.shape[1])\n",
    "\n",
    "theta.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(perm)\n",
      "features indices: [193 143 224   4 184 102  13 110  59 218 190  20  84 196  94  62   7 198\n",
      "  69  39  53 107  70  56 237  66 207 214 166 146 225  68 142  43 222 136\n",
      "  12 152 188  63 208  80 217  18  77 205  85 206 114 178  99 231  81 145\n",
      " 202 132 171  33 216 234  22  16  55  34  78 221  42  89 109 236 168 111\n",
      " 165 242  44  21  95  88  96 155 118  47  54  52  17  57  46  45  58 197\n",
      "  83  97 148 220 173 141 199 191 179 153 158  28  51 127 150 154  37 223\n",
      "  72  86  74 182 213 128 174 157 122 116 189 243 163  26  15  23 192 131\n",
      " 229  36  90 147 185 204  48  49 119 104 209  92  32   6 100  79 170  75\n",
      "  11 133 125 101 121 180 160   8 244 215  40  25  27 219 161 169 117 115\n",
      "  91   5 230 203 105  29  87 211 137 139  50  61  65  71 241  14  10 233\n",
      " 245 167  64  76 164   2  93 123  38 240 200 226  30 156 172 112  41   0\n",
      "  31 239 238 183 149 176 138 130  60 144 162 135 186 181 151 103 228 177\n",
      "   9 106 212 232 235 246 124 126   3  73 175   1 140 227 194 120 159  24\n",
      "  35  98 134  82 201 195 187 210 129 108 113  67  19]\n"
     ]
    }
   ],
   "source": [
    "# shuffle index\n",
    "perm = np.random.permutation(X_cross_train.shape[0])\n",
    "\n",
    "print(f'len(perm)')\n",
    "print(f'features indices: {perm}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_cross_train: 247\n",
      "y_cross_train: 247\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(500):\n",
    "    '''\n",
    "    shuffle the features' indices\n",
    "    with replacement or no replacement\n",
    "    with replacement means just randomize\n",
    "    with no replacement means 0:50, 51:100, 101:150, ......300:323\n",
    "    shuffle your index\n",
    "    '''\n",
    "    X_cross_train = X_cross_train[perm]\n",
    "    y_cross_train = y_cross_train[perm]\n",
    "\n",
    "    print(f'X_cross_train: {len(X_cross_train)}')\n",
    "    print(f'y_cross_train: {len(y_cross_train)}')\n",
    "\n",
    "    # if stochastic batch:\n",
    "    for i in range(X_cross_train.shape[0])\n",
    "\n",
    "\n",
    "    break\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "247"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_cross_train.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stochastic method\n",
    "for i in range(X_cross_train.shape[0]):\n",
    "\n",
    "    X_method_train = X_cross_train[i].reshape(1,-1)\n",
    "    \n",
    "    # print(f'X_method_train: {X_method_train.shape}')\n",
    "    # print(f'X_method_train: {len(X_method_train)}')\n",
    "\n",
    "    y_method_train = y_cross_train[i]\n",
    "\n",
    "    # print(f'y_method_train: {y_method_train}')\n",
    "    # print(f'y_cross_train: {len(y_cross_train)}')\n",
    "\n",
    "    # _train -> Indicates that the name is meant for internal use only\n",
    "    # train_loss = _train(X_method_train, y_method_train)\n",
    "    break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(247, 11)"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_cross_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_method_train: (50, 11)\n",
      "X_method_train: 50\n",
      "y_method_train: (50,)\n",
      "y_method_train: 50\n"
     ]
    }
   ],
   "source": [
    "# mini batch method\n",
    "batch_size = 50\n",
    "for i in range(0, X_cross_train.shape[0]):\n",
    "    '''\n",
    "    0: 50: 247 \n",
    "    #batch_idx = 0, 50, 100, 150\n",
    "    '''\n",
    "    X_method_train = X_cross_train[i:i+batch_size,:]\n",
    "\n",
    "    print(f'X_method_train: {X_method_train.shape}')\n",
    "    print(f'X_method_train: {len(X_method_train)}')\n",
    "\n",
    "    y_method_train = y_cross_train[i:i+batch_size]\n",
    "\n",
    "    print(f'y_method_train: {y_method_train.shape}')\n",
    "    print(f'y_method_train: {len(y_method_train)}')\n",
    "\n",
    "    # train_loss = _train(X_method_train, y_method_train)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "247"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_cross_train.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# full batch method\n",
    "X_method_train = X_cross_train\n",
    "y_method_train = y_cross_train\n",
    "# train_loss = self._train(X_method_train, y_method_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "np.allclose\n",
    "Returns True if two arrays are element-wise equal within a tolerance.\n",
    "\n",
    "'''\n",
    "\n",
    "a = [2,3,4,5,6,7,9,9,9]\n",
    "b = [3,5,6,7,7,8,9,9,9]\n",
    "\n",
    "if np.allclose(a,b):\n",
    "    print('stopped')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, validation loss: 0.5\n",
      "Epoch 1, validation loss: 0.4\n",
      "Epoch 2, validation loss: 0.35\n",
      "Epoch 3, validation loss: 0.34\n",
      "Epoch 4, validation loss: 0.34\n",
      "Early stopping at epoch 4 because loss did not change significantly.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Simulated validation losses over epochs\n",
    "validation_losses = [0.5, 0.4, 0.35, 0.34, 0.34, 0.34]\n",
    "\n",
    "# Initialize previous loss to infinity\n",
    "prev_loss = np.infty\n",
    "\n",
    "for epoch, current_loss in enumerate(validation_losses):\n",
    "    print(f\"Epoch {epoch}, validation loss: {current_loss}\")\n",
    "    \n",
    "    # If the current loss is almost equal to the previous loss, break the loop (early stopping)\n",
    "    if np.allclose(current_loss, prev_loss):\n",
    "        print(f\"Early stopping at epoch {epoch} because loss did not change significantly.\")\n",
    "        break\n",
    "    \n",
    "    # Update previous loss\n",
    "    prev_loss = current_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' \\nsys.modules[__name__]: Gets the current module.\\ngetattr(...): Looks up the attribute (class) with the name provided in the string.\\nUsage: If you have a class named Example defined in the module, calling str_to_class(\"Example\") returns the Example class object.\\n'"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#helper function for looping classnames\n",
    "import sys\n",
    "\n",
    "def str_to_class(classname):\n",
    "    return getattr(sys.modules[__name__], classname)\n",
    "\n",
    "''' \n",
    "sys.modules[__name__]: Gets the current module.\n",
    "getattr(...): Looks up the attribute (class) with the name provided in the string.\n",
    "Usage: If you have a class named Example defined in the module, calling str_to_class(\"Example\") returns the Example class object.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "class A:\n",
    "    def __init__(self):\n",
    "        self.value = \"Instance of Class A\"\n",
    "\n",
    "    def show(self):\n",
    "        print(self.value)\n",
    "\n",
    "class B:\n",
    "    def __init__(self):\n",
    "        self.value = \"Instance of Class B\"\n",
    "\n",
    "    def show(self):\n",
    "        print(self.value)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Instance of Class A\n",
      "Instance of Class B\n"
     ]
    }
   ],
   "source": [
    "# Convert string to class\n",
    "A_class = str_to_class(\"A\")\n",
    "B_class = str_to_class(\"B\")\n",
    "\n",
    "# Create instances\n",
    "a_instance = A_class()\n",
    "b_instance = B_class()\n",
    "\n",
    "# Use the instances\n",
    "a_instance.show()  # Outputs: Instance of Class A\n",
    "b_instance.show()  # Outputs: Instance of Class B\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lasso\n",
    "\n",
    "class Lasso:\n",
    "    def __int__(self, l): # lambda is a constant parameter\n",
    "        self.l = l\n",
    "    \n",
    "    def __call__(self, theta): # allows us to call class as method # theta is a dynamic input\n",
    "        return self.l * np.sum(np.abs(theta))\n",
    "\n",
    "    def derivation(self, theta):\n",
    "        return self.l * np.sign(theta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ridge\n",
    "# Standalone class\n",
    "\n",
    "\n",
    "class Ridge:\n",
    "    def __int__(self, l): # lambda is a constant parameter\n",
    "        self.l = l\n",
    "    \n",
    "    def __call__(self, theta): # allows us to call class as method # theta is a dynamic input\n",
    "        return self.l * np.sum(np.square(theta))\n",
    "\n",
    "    def derivation(self, theta):\n",
    "        return self.l * 2 * theta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "J(\\theta) = \\frac{1}{2}\\sum_{i=1}^m(h_\\theta(x^{(i)}) - y^{(i)})^2 + \\lambda(\\lambda \\sum_{j=1}^n |\\theta_j| + (1 - \\lambda) \\sum_{k=1}^n \\theta_k^2)\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ElasticNet\n",
    "\n",
    "class ElasticNet:\n",
    "    def __int__(self, l, l_ratio): # lambda is a constant parameter\n",
    "        self.l       = l\n",
    "        self.l_ratio = l_ratio\n",
    "    \n",
    "    def __call__(self, theta): # allows us to call class as method # theta is a dynamic input\n",
    "        l1 =  self.l * (1 - self.l_ratio) * np.sum(np.abs(theta))\n",
    "        l2 =  self.l * self.l_ratio * np.sum(np.square(theta))\n",
    "        return (l1 + l2)\n",
    "\n",
    "    def derivation(self, theta):\n",
    "        l1 = self.l * self.l_ratio * np.sign(theta)\n",
    "        l2 = 2 * self.l * (1 - self.l_ratio) * theta\n",
    "        return (l1 + l2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\" \\nRidge Regression:\\nImagine you have many ingredients (predictors) in a recipe and you believe every one of them adds something, even if they are a bit similar. Ridge doesn't remove any ingredients; it just uses them in smaller, balanced amounts. This helps keep the overall mix stable, especially when some ingredients might be too similar.\\n\\nLasso Regression:\\nThink of Lasso like a strict chef who wants to simplify the recipe. The chef decides that only a few key ingredients really matter and completely leaves out the rest. Lasso automatically sets some of the coefficients to zero, effectively removing the less important predictors from the model.\\n\\nElastic Net:\\nElastic Net is like a chef who uses a combination of both approaches. It sometimes reduces the influence of ingredients (like Ridge) and sometimes completely drops them (like Lasso). This approach is useful when you have many ingredients, and some are very similar—you want to keep the good parts of both techniques.\\n\\nIn short, Ridge is best when you want to keep all predictors but control their impact, Lasso is best when you want to simplify by keeping only the important ones, and Elastic Net is a balanced mix that handles situations where predictors are numerous and possibly similar.\\n\""
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "''' \n",
    "Ridge Regression:\n",
    "Imagine you have many ingredients (predictors) in a recipe and you believe every one of them adds something, even if they are a bit similar. Ridge doesn't remove any ingredients; it just uses them in smaller, balanced amounts. This helps keep the overall mix stable, especially when some ingredients might be too similar.\n",
    "\n",
    "Lasso Regression:\n",
    "Think of Lasso like a strict chef who wants to simplify the recipe. The chef decides that only a few key ingredients really matter and completely leaves out the rest. Lasso automatically sets some of the coefficients to zero, effectively removing the less important predictors from the model.\n",
    "\n",
    "Elastic Net:\n",
    "Elastic Net is like a chef who uses a combination of both approaches. It sometimes reduces the influence of ingredients (like Ridge) and sometimes completely drops them (like Lasso). This approach is useful when you have many ingredients, and some are very similar—you want to keep the good parts of both techniques.\n",
    "\n",
    "In short, Ridge is best when you want to keep all predictors but control their impact, Lasso is best when you want to simplify by keeping only the important ones, and Elastic Net is a balanced mix that handles situations where predictors are numerous and possibly similar.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# inhertis linear regression and has separate classes for each of this regularization algorithm\n",
    "\n",
    "class LassoRegression(LinarRegression):\n",
    "    def __init__(self, method, lr, l):\n",
    "        self.regularization = Lasso(l)\n",
    "        super().__init__(self.regularization, lr, method)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parent class\n",
    "\n",
    "class Animal:\n",
    "    def __init__(self, name, breed):\n",
    "        self.name = name\n",
    "        self.breed = breed\n",
    "\n",
    "    def speak(self):\n",
    "        print(\"This animal makes sound\")\n",
    "\n",
    "# Child class\n",
    "\n",
    "class Dog(Animal):\n",
    "    def __init__(self, name, breed):\n",
    "        super().__init__(name, breed)\n",
    "    def speak(self):\n",
    "        print(f'{self.name} says: Woof!!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'buddy'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_dog = Dog('buddy', 'gr')\n",
    "\n",
    "my_dog.name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-1"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "np.sign(-200000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' \\nModel signature defines schema of model input and output.\\n\\nLog a scikit-learn model as an MLflow artifact for the current run.\\n\\n To manually infer a model signature, call infer_signature() on datasets with valid model inputs, \\n such as a training dataset with the target column omitted, and valid model outputs, like model predictions made on the training dataset\\n\\n'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# signature = mlflow.models.infer_signature(X_train, model.predict(X_train))\n",
    "\n",
    "''' \n",
    "Model signature defines schema of model input and output.\n",
    "\n",
    "Log a scikit-learn model as an MLflow artifact for the current run.\n",
    "\n",
    " To manually infer a model signature, call infer_signature() on datasets with valid model inputs, \n",
    " such as a training dataset with the target column omitted, and valid model outputs, like model predictions made on the training dataset\n",
    "\n",
    "'''\n",
    "# mlflow.sklearn.log_model(model, artifact_path='model', signature=signature)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml-dl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
