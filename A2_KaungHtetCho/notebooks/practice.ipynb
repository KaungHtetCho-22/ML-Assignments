{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/02/10 09:45:34 INFO mlflow.tracking.fluent: Experiment with name 'st124092-A2Assignment' does not exist. Creating a new experiment.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Experiment: artifact_location='mlflow-artifacts:/333605939037841023', creation_time=1739155534650, experiment_id='333605939037841023', last_update_time=1739155534650, lifecycle_stage='active', name='st124092-A2Assignment', tags={}>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import mlflow\n",
    "import os\n",
    "\n",
    "\n",
    "mlflow.set_tracking_uri('http://localhost:8080')\n",
    "\n",
    "os.environ['LOGNAME'] = 'st124092'\n",
    "\n",
    "mlflow.set_experiment('st124092-A2Assignment')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_diabetes\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import numpy as np\n",
    "from time import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features: ['age', 'sex', 'bmi', 'bp', 's1', 's2', 's3', 's4', 's5', 's6']\n"
     ]
    }
   ],
   "source": [
    "diabetes = load_diabetes()\n",
    "print(f'Features: {diabetes.feature_names}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "features: (442, 10)\n",
      "target: (442,)\n"
     ]
    }
   ],
   "source": [
    "X = diabetes.data   \n",
    "y = diabetes.target\n",
    "\n",
    "print(f'features: {X.shape}')\n",
    "print(f'target: {y.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "442\n",
      "10\n"
     ]
    }
   ],
   "source": [
    "m = X.shape[0] # samples\n",
    "n = X.shape[1] # features\n",
    "\n",
    "print(m)\n",
    "print(n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train: (309, 10)\n",
      "X_test: (133, 10)\n"
     ]
    }
   ],
   "source": [
    "scaler  = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test  = scaler.transform(X_test)\n",
    "\n",
    "print(f'X_train: {X_train.shape}')\n",
    "print(f'X_test: {X_test.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.],\n",
       "       [1.],\n",
       "       [1.]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.ones((3,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "intercept: (309, 1)\n",
      "X_train: (309, 11)\n",
      "intercept: (133, 1)\n",
      "X_test: (133, 11)\n"
     ]
    }
   ],
   "source": [
    "intercept = np.ones((X_train.shape[0], 1))\n",
    "print(f'intercept: {intercept.shape}')\n",
    "\n",
    "X_train = np.concatenate((intercept, X_train), axis = 1)\n",
    "print(f'X_train: {X_train.shape}')\n",
    "\n",
    "intercept = np.ones((X_test.shape[0], 1))\n",
    "print(f'intercept: {intercept.shape}')\n",
    "\n",
    "X_test = np.concatenate((intercept, X_test), axis = 1)\n",
    "print(f'X_test: {X_test.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KFold(n_splits=5, random_state=None, shuffle=False)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "\n",
    "kfold = KFold(n_splits=5)\n",
    "\n",
    "print(kfold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (4185849342.py, line 24)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[11], line 24\u001b[0;36m\u001b[0m\n\u001b[0;31m    for\u001b[0m\n\u001b[0m        ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "\n",
    "class LinarRegression(object):\n",
    "\n",
    "    kfold = KFold(n_splits=5)\n",
    "\n",
    "    def __init__(self, regularization, lr=0.001, method='batch', num_epoches=500, batch_size=50, cv=kfold):\n",
    "        self.lr             = lr\n",
    "        self.num_epoches    = num_epoches\n",
    "        self.batch_size     = batch_size\n",
    "        self.method         = method\n",
    "        self.cv             = cv\n",
    "        self.regularization = regularization\n",
    "\n",
    "    def mse(self, ytrue, ypred):\n",
    "        return ((ypred - ytrue) ** 2).sum() / ytrue.shape[0]\n",
    "    \n",
    "    def fit(self, X_train, y_train):\n",
    "\n",
    "        self.kfold_scores = list()\n",
    "\n",
    "        self.val_loss_old = np.infty\n",
    "\n",
    "        for \n",
    "\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_cross_train: (247, 11)\n",
      "y_cross_train: [202. 220.  88. 293. 104.  98. 155. 341. 102.  63. 237.  53. 233. 257.\n",
      " 262.  39. 199. 321. 182. 137. 113. 202. 178. 180.  90. 181.  53. 214.\n",
      "  42. 197.  61.  40.  99. 142. 190. 235. 296.  63. 232. 310. 164. 167.\n",
      " 134. 110.  68.  55.  65.  78. 252. 201.  77. 233. 131. 158.  60.  91.\n",
      " 196. 270. 248. 168.  53. 101.  57. 295. 150.  85. 292. 275.  90. 270.\n",
      "  78. 191.  45. 152. 104.  77. 166. 134. 308. 131. 249.  52. 141.  86.\n",
      " 192. 310. 170. 160.  89. 178. 173. 182. 158.  89. 185. 170. 182. 197.\n",
      " 280. 118.  97. 242.  47. 153.  97. 143.  90.  77. 248.  90. 281. 148.\n",
      " 259. 246. 142.  95. 110. 265. 200. 219. 150. 232.  96.  91.  37.  96.\n",
      " 163. 252.  90. 198. 171. 139. 268.  48. 141. 258. 154. 173.  65. 160.\n",
      "  73. 103.  92.  51. 332.  72. 275. 183. 265. 101.  88. 135.  66.  93.\n",
      " 243.  71. 161.  83.  81. 346. 230. 216. 135. 127. 235. 181. 209. 180.\n",
      " 215. 142. 217.  59.  94. 200. 151. 107. 317. 155. 124. 109. 178. 128.\n",
      "  55. 276.  72. 214. 200. 275. 190. 263. 113. 139. 143.  48. 140. 221.\n",
      "  74.  83.  55. 230. 136. 116.  51. 120. 259. 132. 297. 144.  52. 225.\n",
      "  68. 274. 288.  92. 144. 242. 146.  71.  81. 264. 272.  85. 177. 125.\n",
      " 178.  77. 202.  69. 128. 185.  39. 132. 150. 128.  75. 172.  67. 172.\n",
      " 168.  70. 144.  54.  80.  74.  96. 248. 109.]\n",
      "X_cross_val: [[ 1.          0.31093209 -0.9221168  -0.89838189 -1.17311582 -0.74705469\n",
      "  -1.17766168  1.49694126 -1.57927079 -0.89144386  0.82425545]\n",
      " [ 1.         -2.27140195 -0.9221168  -1.56966998 -0.5319127  -1.93618488\n",
      "  -2.11135277  0.50760052 -1.57927079 -0.89144386 -0.09582477]\n",
      " [ 1.          0.46283409 -0.9221168   1.31686882  1.10671752 -0.74705469\n",
      "  -1.06700199 -0.6339465  -0.02291718  1.16419476 -0.09582477]\n",
      " [ 1.          1.82995212 -0.9221168   0.17567906  0.53675919  0.65017329\n",
      "   0.20558439  1.26863186 -0.80109399  0.2073492   0.65696814]\n",
      " [ 1.         -0.06882291  1.08446131 -0.65224292 -0.36591233  0.67990154\n",
      "   0.95945349 -0.32953396  0.45177067  0.22410555  0.57332448]\n",
      " [ 1.         -0.90428393 -0.9221168  -1.32353101 -0.81689186 -0.12276134\n",
      "   0.33007653 -0.93835904  0.75525963  0.15650233 -0.26311208]\n",
      " [ 1.          0.00712809  1.08446131  0.26518414 -0.38942311 -0.36058738\n",
      "  -0.07106483 -1.01446217  0.75525963  0.51628243  0.48968082]\n",
      " [ 1.          1.37424611  1.08446131 -0.71937173 -1.74307416 -0.15248959\n",
      "   0.44765245 -1.16666844  0.75525963  0.0322742   0.65696814]\n",
      " [ 1.         -1.05618593  1.08446131 -0.0704599  -0.95938145 -0.44977214\n",
      "  -0.43762503 -1.47108098  1.53343643  1.30190502 -0.76497402]\n",
      " [ 1.         -0.90428393 -0.9221168  -1.34590728 -0.95938145 -0.8065112\n",
      "  -0.94250985  0.96421932 -0.80109399 -1.17842049 -0.26311208]\n",
      " [ 1.         -0.90428393 -0.9221168   1.02597732  1.24920711  2.76087938\n",
      "   2.84066814 -0.55784336  2.31161324  1.35795214  0.82425545]\n",
      " [ 1.         -0.82833293 -0.9221168  -1.09976832 -1.60058458 -0.71732643\n",
      "  -1.89694963  2.86679768 -1.57927079 -0.39761294 -0.09582477]\n",
      " [ 1.          0.9185401   1.08446131  1.45112644  0.18053523 -0.36058738\n",
      "   0.11567339 -1.62328725  1.53343643  0.69328345 -0.34675574]\n",
      " [ 1.          0.23498109 -0.9221168   0.51132311  1.10671752  0.17452121\n",
      "  -0.61744702  0.73590992 -0.80109399  1.16419476 -0.51404305]\n",
      " [ 1.          0.15903009 -0.9221168  -0.62986665 -0.5319127   0.91772758\n",
      "  -0.21630566  3.24731336 -1.57927079 -0.24333893 -0.76497402]\n",
      " [ 1.         -0.82833293  1.08446131  1.51825525 -1.17311582  3.32571623\n",
      "   3.45621264 -0.02512142  1.54899997  1.0703977   1.40976105]\n",
      " [ 1.          0.7666381   1.08446131  0.15330279  0.67924877  0.14479296\n",
      "   0.39923884 -0.55784336  0.75525963  0.32117684  1.24247373]\n",
      " [ 1.         -0.90428393  1.08446131  0.33231295 -0.10444394 -0.27140261\n",
      "   0.15025454 -1.54718411  1.53343643  0.75722724 -0.26311208]\n",
      " [ 1.          1.45019711  1.08446131 -0.60749038  0.46551439 -0.8065112\n",
      "  -1.02550461  0.65980679 -0.80109399 -0.30805312 -0.01218112]\n",
      " [ 1.         -0.37262692  1.08446131  0.08617398  0.32302481  0.47180376\n",
      "  -0.20938943 -0.55784336  0.75525963  1.81518873  1.66069202]\n",
      " [ 1.         -2.27140195 -0.9221168  -0.20471752 -0.81689186  1.06636886\n",
      "   1.43667338 -0.32953396  0.75525963  0.15650233 -0.59768671]\n",
      " [ 1.         -0.90428393 -0.9221168   0.8917197  -0.5319127  -0.68759818\n",
      "  -0.6589444  -0.78615277  0.09380934  0.71909209 -0.34675574]\n",
      " [ 1.          0.00712809  1.08446131  1.27211628 -0.03319915  1.33392315\n",
      "   1.40900846 -1.24277157  2.31161324  1.46561652  2.58077224]\n",
      " [ 1.          1.1463931   1.08446131 -0.60749038  0.18053523  0.41234725\n",
      "   0.4822336   0.65980679 -0.80109399 -0.61583074 -1.18319231]\n",
      " [ 1.         -1.13213693 -0.9221168   1.51825525 -1.52933979 -0.33085912\n",
      "  -0.01573498  0.05098171 -0.41978735 -0.9825445   0.07146254]\n",
      " [ 1.          0.9185401   1.08446131 -0.02570736  2.2466342   0.76908631\n",
      "   0.50989852  0.50760052 -0.02291718  0.6000642   0.40603717]\n",
      " [ 1.          1.52614811 -0.9221168   1.80914676  1.3204519   0.32316249\n",
      "   0.30241161  0.27929111 -0.02291718  0.13916818  1.74433567]\n",
      " [ 1.         -0.14477391 -0.9221168   0.93647224  1.81916544  0.29343423\n",
      "   0.17100324 -0.32953396 -0.02291718  0.90264157 -0.34675574]\n",
      " [ 1.         -0.98023493 -0.9221168   0.84696716  0.03804564  0.35289074\n",
      "   0.35082522 -0.6339465   0.5918425   0.94886599 -0.51404305]\n",
      " [ 1.         -1.05618593 -0.9221168   0.13092652  0.25178002 -0.8065112\n",
      "  -0.39612765 -0.40563709 -0.02291718 -0.83308553 -0.4303994 ]\n",
      " [ 1.         -0.82833293 -0.9221168   0.24280787 -0.95938145 -1.28216328\n",
      "  -1.15691299 -0.25343083 -0.80109399 -0.33000972 -0.84861768]\n",
      " [ 1.          0.31093209 -0.9221168  -0.56273784 -0.17568874 -0.09303308\n",
      "  -0.2024732   0.88811619 -0.80109399 -0.64163938 -0.84861768]\n",
      " [ 1.          1.52614811  1.08446131 -0.38372768  0.96422794  1.60147744\n",
      "   0.73813412 -0.78615277  1.53343643  2.25509115  0.74061179]\n",
      " [ 1.          0.46283409  1.08446131 -0.36135141  1.03547273 -1.37134804\n",
      "  -1.33673498  0.05098171 -0.80109399 -0.54033085 -0.26311208]\n",
      " [ 1.          0.38688309 -0.9221168  -1.16689713 -1.17311582 -0.03357657\n",
      "  -0.26471928  1.49694126 -0.80109399 -1.28454406 -1.51776693]\n",
      " [ 1.          0.08307909 -0.9221168  -0.9655107  -0.24693353  0.02587994\n",
      "  -0.1333109   1.26863186 -0.80109399 -1.07807497 -1.18319231]\n",
      " [ 1.          0.8425891   1.08446131 -0.42848022  0.60800398 -1.43080455\n",
      "  -0.98400723 -1.31887471 -0.02291718  0.06848333 -1.09954865]\n",
      " [ 1.         -0.14477391 -0.9221168   1.38399763  0.75049356 -0.03357657\n",
      "   0.34390899 -0.32953396  0.04711873 -0.37469333  0.65696814]\n",
      " [ 1.         -1.13213693 -0.9221168   0.01904517 -0.5319127   0.23397772\n",
      "   0.17100324  0.96421932 -0.80109399 -0.74872596  0.40603717]\n",
      " [ 1.         -0.06882291 -0.9221168  -1.34590728  0.03804564 -0.06330483\n",
      "   0.08109224  0.20318798 -0.02291718 -0.42072515 -0.51404305]\n",
      " [ 1.          0.31093209 -0.9221168  -0.94313443 -0.03319915 -0.42004389\n",
      "  -0.93559362  0.65980679 -0.80109399  0.58619687  0.57332448]\n",
      " [ 1.         -0.82833293  1.08446131  0.13092652  0.75049356 -0.24167436\n",
      "   0.14333831 -1.01446217  0.75525963  0.35257093 -0.01218112]\n",
      " [ 1.          0.31093209 -0.9221168   0.06379771  0.18053523  0.1150647\n",
      "   0.253998    0.58370365 -0.80109399 -0.95172822  0.65696814]\n",
      " [ 1.          1.45019711 -0.9221168   1.47350271  0.79822757  0.47180376\n",
      "   0.04651109 -0.78615277  0.85642261  1.6483956   2.16255395]\n",
      " [ 1.         -1.20808793  1.08446131 -0.18234125  0.53675919  0.50153201\n",
      "   0.89720742 -0.8622559   0.75525963  0.44347896  1.15883008]\n",
      " [ 1.         -0.22072492 -0.9221168  -0.2942226   1.46294148  0.26370598\n",
      "   0.50298229  0.27929111 -0.02291718 -0.69441226  0.99154276]\n",
      " [ 1.         -1.13213693 -0.9221168  -1.2564022  -0.5319127  -0.12276134\n",
      "  -0.09872975  0.58370365 -0.80109399 -0.64163938 -1.43412328]\n",
      " [ 1.         -0.52452892  1.08446131 -0.49560903  0.89298315 -1.16325026\n",
      "  -1.03933707 -0.48174023 -0.80109399  0.0322742   0.40603717]\n",
      " [ 1.          1.0704421  -0.9221168  -0.40610395 -1.52933979 -0.86596771\n",
      "  -0.8180177  -0.17732769 -0.80109399 -0.00489794 -1.09954865]\n",
      " [ 1.          1.22234411  1.08446131  0.8917197   0.25178002  0.88799933\n",
      "   1.57499799 -0.78615277  0.75525963 -0.22234534 -0.59768671]\n",
      " [ 1.         -1.13213693  1.08446131 -0.80887681 -1.3868502  -0.68759818\n",
      "  -0.80418524  0.73590992 -0.80109399 -0.72137651  1.40976105]\n",
      " [ 1.          0.6147361  -0.9221168  -0.65224292 -0.4606679  -1.01460898\n",
      "  -1.77937372  2.10576634 -1.57927079 -0.83308553 -1.09954865]\n",
      " [ 1.         -0.14477391 -0.9221168   0.2204316  -1.03062624  0.56098852\n",
      "   1.05628072 -0.93835904  1.53343643  0.32117684  0.07146254]\n",
      " [ 1.         -1.81569594  1.08446131 -0.60749038 -0.4606679  -1.37134804\n",
      "  -1.18457791  0.35539425 -0.80109399 -2.02952769 -0.68133037]\n",
      " [ 1.          0.7666381   1.08446131 -0.58511411 -1.52933979 -0.27140261\n",
      "  -0.26471928  0.05098171 -0.02291718  0.08639529 -0.59768671]\n",
      " [ 1.         -0.52452892 -0.9221168  -0.2942226  -0.24693353  0.44207551\n",
      "   0.92487234 -0.93835904  0.75525963  0.3053835  -0.17946843]\n",
      " [ 1.          0.9944911   1.08446131 -0.60749038 -1.00711546  1.06636886\n",
      "  -0.07798106  2.71459141 -1.09680117  0.4583093   0.40603717]\n",
      " [ 1.          0.31093209 -0.9221168   0.57845192  1.22569633 -1.31189153\n",
      "  -0.96325854 -0.32953396 -0.68436747 -1.07807497 -0.51404305]\n",
      " [ 1.          0.15903009  1.08446131  0.667957    0.89298315 -0.06330483\n",
      "   0.82804512 -1.39497784  1.53343643 -0.4916026   0.32239351]\n",
      " [ 1.          0.00712809  1.08446131 -0.09283617 -0.38942311 -0.2119461\n",
      "   0.12258962 -0.8622559   0.75525963  0.32117684  1.99526664]\n",
      " [ 1.         -1.43594093  1.08446131 -0.22709379 -0.81689186 -0.33085912\n",
      "   0.11567339 -1.24277157  0.75525963  0.41362569 -0.68133037]\n",
      " [ 1.          0.00712809  1.08446131  0.57845192 -0.17568874  0.53126027\n",
      "   0.86262627 -0.48174023  0.75525963  0.2073492   0.07146254]]\n",
      "y_cross_val: [ 93. 137. 281. 109.  84. 214. 111. 102. 126.  59. 258.  60. 259. 265.\n",
      "  87. 220. 311. 142.  66. 123. 200.  52. 268.  91.  55. 217. 261. 195.\n",
      " 220. 200. 210. 179. 241. 219.  63.  96. 156. 109. 113.  79. 200. 107.\n",
      "  49. 220. 179. 104. 115. 252. 131. 198.  42.  65. 174.  43.  88. 151.\n",
      " 102.  52. 244. 262. 202. 196.]\n"
     ]
    }
   ],
   "source": [
    "cv = kfold\n",
    "''' \n",
    "  Generate indices to split data into training and test set\n",
    "  k = training --> 247\n",
    "  v = test --> 62\n",
    "  total samples = 309 / 5 = 62 \n",
    "''' \n",
    "for i, (k,v) in enumerate(cv.split(X_train)): \n",
    "\n",
    "    # print(f'i: {i}')\n",
    "    # print(f'k: {len(k)}')\n",
    "    # print(f'k: {k}')\n",
    "\n",
    "    # print(f'v: {len(v)}')    \n",
    "    # print(f'v: {v}')    \n",
    "\n",
    "    X_cross_train = X_train[k]\n",
    "    print(f'X_cross_train: {X_cross_train.shape}')\n",
    "    y_cross_train = y_train[k]\n",
    "    print(f'y_cross_train: {y_cross_train}')\n",
    "\n",
    "    X_cross_val = X_train[v]\n",
    "    print(f'X_cross_val: {X_cross_val}')\n",
    "    y_cross_val = y_train[v]\n",
    "    print(f'y_cross_val: {y_cross_val}')\n",
    "\n",
    "    break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(11,)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "theta = np.zeros(X_cross_train.shape[1])\n",
    "\n",
    "theta.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(perm)\n",
      "features indices: [205  78 176  52 138 168 209   5  56 153  33 132  85 197  55  18 180  82\n",
      " 219 122 128 107 206 229 181  21 149 199 193 145  13  79 135 125 109 235\n",
      "  76  36 228  53 154 100 134 227  70  95  64 115 213 124  29  32 159 201\n",
      " 112 196   7 111  66 236 202 165 101 179  91 238 234  65 102 133 123  97\n",
      " 184 187 190 161  94  17 237  34 167 164 232  98 173 156  61  31  80  23\n",
      "  62  69 218  88 242 170  19  44  45  72 104 225  30 126  35 129 226 211\n",
      "  54 119 223   6 192  39 120  24 224  74   8  73   9 116 144 240  14 146\n",
      "  50 246  16 177  87 186 142   4  67 183 106 114  41  77  75 239 103  11\n",
      "  84 182 127 215 152 140 208 203 158  90 204 166 200  47 198  63 233 210\n",
      " 189  38  92 241  58  68  27  26  46 188 157  10  37 150 195  81 230 244\n",
      "   1 194 172  42  40 174 136  96   0 171  89 117 137 139  12 214 118 212\n",
      " 113 220  86  25 169  99 191 143 243  22 185 222  83 121  93 175  51 216\n",
      " 245 207  60 108  15  20  59 163 160 221 131  57   2  71 217 151 130 178\n",
      "  43 141 231 110 105 147 155 148  49  28 162  48   3]\n"
     ]
    }
   ],
   "source": [
    "# shuffle index\n",
    "perm = np.random.permutation(X_cross_train.shape[0])\n",
    "\n",
    "print(f'len(perm)')\n",
    "print(f'features indices: {perm}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_cross_train: 247\n",
      "y_cross_train: 247\n",
      "X_cross_train: 247\n",
      "y_cross_train: 247\n",
      "X_cross_train: 247\n",
      "y_cross_train: 247\n",
      "X_cross_train: 247\n",
      "y_cross_train: 247\n",
      "X_cross_train: 247\n",
      "y_cross_train: 247\n",
      "X_cross_train: 247\n",
      "y_cross_train: 247\n",
      "X_cross_train: 247\n",
      "y_cross_train: 247\n",
      "X_cross_train: 247\n",
      "y_cross_train: 247\n",
      "X_cross_train: 247\n",
      "y_cross_train: 247\n",
      "X_cross_train: 247\n",
      "y_cross_train: 247\n",
      "X_cross_train: 247\n",
      "y_cross_train: 247\n",
      "X_cross_train: 247\n",
      "y_cross_train: 247\n",
      "X_cross_train: 247\n",
      "y_cross_train: 247\n",
      "X_cross_train: 247\n",
      "y_cross_train: 247\n",
      "X_cross_train: 247\n",
      "y_cross_train: 247\n",
      "X_cross_train: 247\n",
      "y_cross_train: 247\n",
      "X_cross_train: 247\n",
      "y_cross_train: 247\n",
      "X_cross_train: 247\n",
      "y_cross_train: 247\n",
      "X_cross_train: 247\n",
      "y_cross_train: 247\n",
      "X_cross_train: 247\n",
      "y_cross_train: 247\n",
      "X_cross_train: 247\n",
      "y_cross_train: 247\n",
      "X_cross_train: 247\n",
      "y_cross_train: 247\n",
      "X_cross_train: 247\n",
      "y_cross_train: 247\n",
      "X_cross_train: 247\n",
      "y_cross_train: 247\n",
      "X_cross_train: 247\n",
      "y_cross_train: 247\n",
      "X_cross_train: 247\n",
      "y_cross_train: 247\n",
      "X_cross_train: 247\n",
      "y_cross_train: 247\n",
      "X_cross_train: 247\n",
      "y_cross_train: 247\n",
      "X_cross_train: 247\n",
      "y_cross_train: 247\n",
      "X_cross_train: 247\n",
      "y_cross_train: 247\n",
      "X_cross_train: 247\n",
      "y_cross_train: 247\n",
      "X_cross_train: 247\n",
      "y_cross_train: 247\n",
      "X_cross_train: 247\n",
      "y_cross_train: 247\n",
      "X_cross_train: 247\n",
      "y_cross_train: 247\n",
      "X_cross_train: 247\n",
      "y_cross_train: 247\n",
      "X_cross_train: 247\n",
      "y_cross_train: 247\n",
      "X_cross_train: 247\n",
      "y_cross_train: 247\n",
      "X_cross_train: 247\n",
      "y_cross_train: 247\n",
      "X_cross_train: 247\n",
      "y_cross_train: 247\n",
      "X_cross_train: 247\n",
      "y_cross_train: 247\n",
      "X_cross_train: 247\n",
      "y_cross_train: 247\n",
      "X_cross_train: 247\n",
      "y_cross_train: 247\n",
      "X_cross_train: 247\n",
      "y_cross_train: 247\n",
      "X_cross_train: 247\n",
      "y_cross_train: 247\n",
      "X_cross_train: 247\n",
      "y_cross_train: 247\n",
      "X_cross_train: 247\n",
      "y_cross_train: 247\n",
      "X_cross_train: 247\n",
      "y_cross_train: 247\n",
      "X_cross_train: 247\n",
      "y_cross_train: 247\n",
      "X_cross_train: 247\n",
      "y_cross_train: 247\n",
      "X_cross_train: 247\n",
      "y_cross_train: 247\n",
      "X_cross_train: 247\n",
      "y_cross_train: 247\n",
      "X_cross_train: 247\n",
      "y_cross_train: 247\n",
      "X_cross_train: 247\n",
      "y_cross_train: 247\n",
      "X_cross_train: 247\n",
      "y_cross_train: 247\n",
      "X_cross_train: 247\n",
      "y_cross_train: 247\n",
      "X_cross_train: 247\n",
      "y_cross_train: 247\n",
      "X_cross_train: 247\n",
      "y_cross_train: 247\n",
      "X_cross_train: 247\n",
      "y_cross_train: 247\n",
      "X_cross_train: 247\n",
      "y_cross_train: 247\n",
      "X_cross_train: 247\n",
      "y_cross_train: 247\n",
      "X_cross_train: 247\n",
      "y_cross_train: 247\n",
      "X_cross_train: 247\n",
      "y_cross_train: 247\n",
      "X_cross_train: 247\n",
      "y_cross_train: 247\n",
      "X_cross_train: 247\n",
      "y_cross_train: 247\n",
      "X_cross_train: 247\n",
      "y_cross_train: 247\n",
      "X_cross_train: 247\n",
      "y_cross_train: 247\n",
      "X_cross_train: 247\n",
      "y_cross_train: 247\n",
      "X_cross_train: 247\n",
      "y_cross_train: 247\n",
      "X_cross_train: 247\n",
      "y_cross_train: 247\n",
      "X_cross_train: 247\n",
      "y_cross_train: 247\n",
      "X_cross_train: 247\n",
      "y_cross_train: 247\n",
      "X_cross_train: 247\n",
      "y_cross_train: 247\n",
      "X_cross_train: 247\n",
      "y_cross_train: 247\n",
      "X_cross_train: 247\n",
      "y_cross_train: 247\n",
      "X_cross_train: 247\n",
      "y_cross_train: 247\n",
      "X_cross_train: 247\n",
      "y_cross_train: 247\n",
      "X_cross_train: 247\n",
      "y_cross_train: 247\n",
      "X_cross_train: 247\n",
      "y_cross_train: 247\n",
      "X_cross_train: 247\n",
      "y_cross_train: 247\n",
      "X_cross_train: 247\n",
      "y_cross_train: 247\n",
      "X_cross_train: 247\n",
      "y_cross_train: 247\n",
      "X_cross_train: 247\n",
      "y_cross_train: 247\n",
      "X_cross_train: 247\n",
      "y_cross_train: 247\n",
      "X_cross_train: 247\n",
      "y_cross_train: 247\n",
      "X_cross_train: 247\n",
      "y_cross_train: 247\n",
      "X_cross_train: 247\n",
      "y_cross_train: 247\n",
      "X_cross_train: 247\n",
      "y_cross_train: 247\n",
      "X_cross_train: 247\n",
      "y_cross_train: 247\n",
      "X_cross_train: 247\n",
      "y_cross_train: 247\n",
      "X_cross_train: 247\n",
      "y_cross_train: 247\n",
      "X_cross_train: 247\n",
      "y_cross_train: 247\n",
      "X_cross_train: 247\n",
      "y_cross_train: 247\n",
      "X_cross_train: 247\n",
      "y_cross_train: 247\n",
      "X_cross_train: 247\n",
      "y_cross_train: 247\n",
      "X_cross_train: 247\n",
      "y_cross_train: 247\n",
      "X_cross_train: 247\n",
      "y_cross_train: 247\n",
      "X_cross_train: 247\n",
      "y_cross_train: 247\n",
      "X_cross_train: 247\n",
      "y_cross_train: 247\n",
      "X_cross_train: 247\n",
      "y_cross_train: 247\n",
      "X_cross_train: 247\n",
      "y_cross_train: 247\n",
      "X_cross_train: 247\n",
      "y_cross_train: 247\n",
      "X_cross_train: 247\n",
      "y_cross_train: 247\n",
      "X_cross_train: 247\n",
      "y_cross_train: 247\n",
      "X_cross_train: 247\n",
      "y_cross_train: 247\n",
      "X_cross_train: 247\n",
      "y_cross_train: 247\n",
      "X_cross_train: 247\n",
      "y_cross_train: 247\n",
      "X_cross_train: 247\n",
      "y_cross_train: 247\n",
      "X_cross_train: 247\n",
      "y_cross_train: 247\n",
      "X_cross_train: 247\n",
      "y_cross_train: 247\n",
      "X_cross_train: 247\n",
      "y_cross_train: 247\n",
      "X_cross_train: 247\n",
      "y_cross_train: 247\n",
      "X_cross_train: 247\n",
      "y_cross_train: 247\n",
      "X_cross_train: 247\n",
      "y_cross_train: 247\n",
      "X_cross_train: 247\n",
      "y_cross_train: 247\n",
      "X_cross_train: 247\n",
      "y_cross_train: 247\n",
      "X_cross_train: 247\n",
      "y_cross_train: 247\n",
      "X_cross_train: 247\n",
      "y_cross_train: 247\n",
      "X_cross_train: 247\n",
      "y_cross_train: 247\n",
      "X_cross_train: 247\n",
      "y_cross_train: 247\n",
      "X_cross_train: 247\n",
      "y_cross_train: 247\n",
      "X_cross_train: 247\n",
      "y_cross_train: 247\n",
      "X_cross_train: 247\n",
      "y_cross_train: 247\n",
      "X_cross_train: 247\n",
      "y_cross_train: 247\n",
      "X_cross_train: 247\n",
      "y_cross_train: 247\n",
      "X_cross_train: 247\n",
      "y_cross_train: 247\n",
      "X_cross_train: 247\n",
      "y_cross_train: 247\n",
      "X_cross_train: 247\n",
      "y_cross_train: 247\n",
      "X_cross_train: 247\n",
      "y_cross_train: 247\n",
      "X_cross_train: 247\n",
      "y_cross_train: 247\n",
      "X_cross_train: 247\n",
      "y_cross_train: 247\n",
      "X_cross_train: 247\n",
      "y_cross_train: 247\n",
      "X_cross_train: 247\n",
      "y_cross_train: 247\n",
      "X_cross_train: 247\n",
      "y_cross_train: 247\n",
      "X_cross_train: 247\n",
      "y_cross_train: 247\n",
      "X_cross_train: 247\n",
      "y_cross_train: 247\n",
      "X_cross_train: 247\n",
      "y_cross_train: 247\n",
      "X_cross_train: 247\n",
      "y_cross_train: 247\n",
      "X_cross_train: 247\n",
      "y_cross_train: 247\n",
      "X_cross_train: 247\n",
      "y_cross_train: 247\n",
      "X_cross_train: 247\n",
      "y_cross_train: 247\n",
      "X_cross_train: 247\n",
      "y_cross_train: 247\n",
      "X_cross_train: 247\n",
      "y_cross_train: 247\n",
      "X_cross_train: 247\n",
      "y_cross_train: 247\n",
      "X_cross_train: 247\n",
      "y_cross_train: 247\n",
      "X_cross_train: 247\n",
      "y_cross_train: 247\n",
      "X_cross_train: 247\n",
      "y_cross_train: 247\n",
      "X_cross_train: 247\n",
      "y_cross_train: 247\n",
      "X_cross_train: 247\n",
      "y_cross_train: 247\n",
      "X_cross_train: 247\n",
      "y_cross_train: 247\n",
      "X_cross_train: 247\n",
      "y_cross_train: 247\n",
      "X_cross_train: 247\n",
      "y_cross_train: 247\n",
      "X_cross_train: 247\n",
      "y_cross_train: 247\n",
      "X_cross_train: 247\n",
      "y_cross_train: 247\n",
      "X_cross_train: 247\n",
      "y_cross_train: 247\n",
      "X_cross_train: 247\n",
      "y_cross_train: 247\n",
      "X_cross_train: 247\n",
      "y_cross_train: 247\n",
      "X_cross_train: 247\n",
      "y_cross_train: 247\n",
      "X_cross_train: 247\n",
      "y_cross_train: 247\n",
      "X_cross_train: 247\n",
      "y_cross_train: 247\n",
      "X_cross_train: 247\n",
      "y_cross_train: 247\n",
      "X_cross_train: 247\n",
      "y_cross_train: 247\n",
      "X_cross_train: 247\n",
      "y_cross_train: 247\n",
      "X_cross_train: 247\n",
      "y_cross_train: 247\n",
      "X_cross_train: 247\n",
      "y_cross_train: 247\n",
      "X_cross_train: 247\n",
      "y_cross_train: 247\n",
      "X_cross_train: 247\n",
      "y_cross_train: 247\n",
      "X_cross_train: 247\n",
      "y_cross_train: 247\n",
      "X_cross_train: 247\n",
      "y_cross_train: 247\n",
      "X_cross_train: 247\n",
      "y_cross_train: 247\n",
      "X_cross_train: 247\n",
      "y_cross_train: 247\n",
      "X_cross_train: 247\n",
      "y_cross_train: 247\n",
      "X_cross_train: 247\n",
      "y_cross_train: 247\n",
      "X_cross_train: 247\n",
      "y_cross_train: 247\n",
      "X_cross_train: 247\n",
      "y_cross_train: 247\n",
      "X_cross_train: 247\n",
      "y_cross_train: 247\n",
      "X_cross_train: 247\n",
      "y_cross_train: 247\n",
      "X_cross_train: 247\n",
      "y_cross_train: 247\n",
      "X_cross_train: 247\n",
      "y_cross_train: 247\n",
      "X_cross_train: 247\n",
      "y_cross_train: 247\n",
      "X_cross_train: 247\n",
      "y_cross_train: 247\n",
      "X_cross_train: 247\n",
      "y_cross_train: 247\n",
      "X_cross_train: 247\n",
      "y_cross_train: 247\n",
      "X_cross_train: 247\n",
      "y_cross_train: 247\n",
      "X_cross_train: 247\n",
      "y_cross_train: 247\n",
      "X_cross_train: 247\n",
      "y_cross_train: 247\n",
      "X_cross_train: 247\n",
      "y_cross_train: 247\n",
      "X_cross_train: 247\n",
      "y_cross_train: 247\n",
      "X_cross_train: 247\n",
      "y_cross_train: 247\n",
      "X_cross_train: 247\n",
      "y_cross_train: 247\n",
      "X_cross_train: 247\n",
      "y_cross_train: 247\n",
      "X_cross_train: 247\n",
      "y_cross_train: 247\n",
      "X_cross_train: 247\n",
      "y_cross_train: 247\n",
      "X_cross_train: 247\n",
      "y_cross_train: 247\n",
      "X_cross_train: 247\n",
      "y_cross_train: 247\n",
      "X_cross_train: 247\n",
      "y_cross_train: 247\n",
      "X_cross_train: 247\n",
      "y_cross_train: 247\n",
      "X_cross_train: 247\n",
      "y_cross_train: 247\n",
      "X_cross_train: 247\n",
      "y_cross_train: 247\n",
      "X_cross_train: 247\n",
      "y_cross_train: 247\n",
      "X_cross_train: 247\n",
      "y_cross_train: 247\n",
      "X_cross_train: 247\n",
      "y_cross_train: 247\n",
      "X_cross_train: 247\n",
      "y_cross_train: 247\n",
      "X_cross_train: 247\n",
      "y_cross_train: 247\n",
      "X_cross_train: 247\n",
      "y_cross_train: 247\n",
      "X_cross_train: 247\n",
      "y_cross_train: 247\n",
      "X_cross_train: 247\n",
      "y_cross_train: 247\n",
      "X_cross_train: 247\n",
      "y_cross_train: 247\n",
      "X_cross_train: 247\n",
      "y_cross_train: 247\n",
      "X_cross_train: 247\n",
      "y_cross_train: 247\n",
      "X_cross_train: 247\n",
      "y_cross_train: 247\n",
      "X_cross_train: 247\n",
      "y_cross_train: 247\n",
      "X_cross_train: 247\n",
      "y_cross_train: 247\n",
      "X_cross_train: 247\n",
      "y_cross_train: 247\n",
      "X_cross_train: 247\n",
      "y_cross_train: 247\n",
      "X_cross_train: 247\n",
      "y_cross_train: 247\n",
      "X_cross_train: 247\n",
      "y_cross_train: 247\n",
      "X_cross_train: 247\n",
      "y_cross_train: 247\n",
      "X_cross_train: 247\n",
      "y_cross_train: 247\n",
      "X_cross_train: 247\n",
      "y_cross_train: 247\n",
      "X_cross_train: 247\n",
      "y_cross_train: 247\n",
      "X_cross_train: 247\n",
      "y_cross_train: 247\n",
      "X_cross_train: 247\n",
      "y_cross_train: 247\n",
      "X_cross_train: 247\n",
      "y_cross_train: 247\n",
      "X_cross_train: 247\n",
      "y_cross_train: 247\n",
      "X_cross_train: 247\n",
      "y_cross_train: 247\n",
      "X_cross_train: 247\n",
      "y_cross_train: 247\n",
      "X_cross_train: 247\n",
      "y_cross_train: 247\n",
      "X_cross_train: 247\n",
      "y_cross_train: 247\n",
      "X_cross_train: 247\n",
      "y_cross_train: 247\n",
      "X_cross_train: 247\n",
      "y_cross_train: 247\n",
      "X_cross_train: 247\n",
      "y_cross_train: 247\n",
      "X_cross_train: 247\n",
      "y_cross_train: 247\n",
      "X_cross_train: 247\n",
      "y_cross_train: 247\n",
      "X_cross_train: 247\n",
      "y_cross_train: 247\n",
      "X_cross_train: 247\n",
      "y_cross_train: 247\n",
      "X_cross_train: 247\n",
      "y_cross_train: 247\n",
      "X_cross_train: 247\n",
      "y_cross_train: 247\n",
      "X_cross_train: 247\n",
      "y_cross_train: 247\n",
      "X_cross_train: 247\n",
      "y_cross_train: 247\n",
      "X_cross_train: 247\n",
      "y_cross_train: 247\n",
      "X_cross_train: 247\n",
      "y_cross_train: 247\n",
      "X_cross_train: 247\n",
      "y_cross_train: 247\n",
      "X_cross_train: 247\n",
      "y_cross_train: 247\n",
      "X_cross_train: 247\n",
      "y_cross_train: 247\n",
      "X_cross_train: 247\n",
      "y_cross_train: 247\n",
      "X_cross_train: 247\n",
      "y_cross_train: 247\n",
      "X_cross_train: 247\n",
      "y_cross_train: 247\n",
      "X_cross_train: 247\n",
      "y_cross_train: 247\n",
      "X_cross_train: 247\n",
      "y_cross_train: 247\n",
      "X_cross_train: 247\n",
      "y_cross_train: 247\n",
      "X_cross_train: 247\n",
      "y_cross_train: 247\n",
      "X_cross_train: 247\n",
      "y_cross_train: 247\n",
      "X_cross_train: 247\n",
      "y_cross_train: 247\n",
      "X_cross_train: 247\n",
      "y_cross_train: 247\n",
      "X_cross_train: 247\n",
      "y_cross_train: 247\n",
      "X_cross_train: 247\n",
      "y_cross_train: 247\n",
      "X_cross_train: 247\n",
      "y_cross_train: 247\n",
      "X_cross_train: 247\n",
      "y_cross_train: 247\n",
      "X_cross_train: 247\n",
      "y_cross_train: 247\n",
      "X_cross_train: 247\n",
      "y_cross_train: 247\n",
      "X_cross_train: 247\n",
      "y_cross_train: 247\n",
      "X_cross_train: 247\n",
      "y_cross_train: 247\n",
      "X_cross_train: 247\n",
      "y_cross_train: 247\n",
      "X_cross_train: 247\n",
      "y_cross_train: 247\n",
      "X_cross_train: 247\n",
      "y_cross_train: 247\n",
      "X_cross_train: 247\n",
      "y_cross_train: 247\n",
      "X_cross_train: 247\n",
      "y_cross_train: 247\n",
      "X_cross_train: 247\n",
      "y_cross_train: 247\n",
      "X_cross_train: 247\n",
      "y_cross_train: 247\n",
      "X_cross_train: 247\n",
      "y_cross_train: 247\n",
      "X_cross_train: 247\n",
      "y_cross_train: 247\n",
      "X_cross_train: 247\n",
      "y_cross_train: 247\n",
      "X_cross_train: 247\n",
      "y_cross_train: 247\n",
      "X_cross_train: 247\n",
      "y_cross_train: 247\n",
      "X_cross_train: 247\n",
      "y_cross_train: 247\n",
      "X_cross_train: 247\n",
      "y_cross_train: 247\n",
      "X_cross_train: 247\n",
      "y_cross_train: 247\n",
      "X_cross_train: 247\n",
      "y_cross_train: 247\n",
      "X_cross_train: 247\n",
      "y_cross_train: 247\n",
      "X_cross_train: 247\n",
      "y_cross_train: 247\n",
      "X_cross_train: 247\n",
      "y_cross_train: 247\n",
      "X_cross_train: 247\n",
      "y_cross_train: 247\n",
      "X_cross_train: 247\n",
      "y_cross_train: 247\n",
      "X_cross_train: 247\n",
      "y_cross_train: 247\n",
      "X_cross_train: 247\n",
      "y_cross_train: 247\n",
      "X_cross_train: 247\n",
      "y_cross_train: 247\n",
      "X_cross_train: 247\n",
      "y_cross_train: 247\n",
      "X_cross_train: 247\n",
      "y_cross_train: 247\n",
      "X_cross_train: 247\n",
      "y_cross_train: 247\n",
      "X_cross_train: 247\n",
      "y_cross_train: 247\n",
      "X_cross_train: 247\n",
      "y_cross_train: 247\n",
      "X_cross_train: 247\n",
      "y_cross_train: 247\n",
      "X_cross_train: 247\n",
      "y_cross_train: 247\n",
      "X_cross_train: 247\n",
      "y_cross_train: 247\n",
      "X_cross_train: 247\n",
      "y_cross_train: 247\n",
      "X_cross_train: 247\n",
      "y_cross_train: 247\n",
      "X_cross_train: 247\n",
      "y_cross_train: 247\n",
      "X_cross_train: 247\n",
      "y_cross_train: 247\n",
      "X_cross_train: 247\n",
      "y_cross_train: 247\n",
      "X_cross_train: 247\n",
      "y_cross_train: 247\n",
      "X_cross_train: 247\n",
      "y_cross_train: 247\n",
      "X_cross_train: 247\n",
      "y_cross_train: 247\n",
      "X_cross_train: 247\n",
      "y_cross_train: 247\n",
      "X_cross_train: 247\n",
      "y_cross_train: 247\n",
      "X_cross_train: 247\n",
      "y_cross_train: 247\n",
      "X_cross_train: 247\n",
      "y_cross_train: 247\n",
      "X_cross_train: 247\n",
      "y_cross_train: 247\n",
      "X_cross_train: 247\n",
      "y_cross_train: 247\n",
      "X_cross_train: 247\n",
      "y_cross_train: 247\n",
      "X_cross_train: 247\n",
      "y_cross_train: 247\n",
      "X_cross_train: 247\n",
      "y_cross_train: 247\n",
      "X_cross_train: 247\n",
      "y_cross_train: 247\n",
      "X_cross_train: 247\n",
      "y_cross_train: 247\n",
      "X_cross_train: 247\n",
      "y_cross_train: 247\n",
      "X_cross_train: 247\n",
      "y_cross_train: 247\n",
      "X_cross_train: 247\n",
      "y_cross_train: 247\n",
      "X_cross_train: 247\n",
      "y_cross_train: 247\n",
      "X_cross_train: 247\n",
      "y_cross_train: 247\n",
      "X_cross_train: 247\n",
      "y_cross_train: 247\n",
      "X_cross_train: 247\n",
      "y_cross_train: 247\n",
      "X_cross_train: 247\n",
      "y_cross_train: 247\n",
      "X_cross_train: 247\n",
      "y_cross_train: 247\n",
      "X_cross_train: 247\n",
      "y_cross_train: 247\n",
      "X_cross_train: 247\n",
      "y_cross_train: 247\n",
      "X_cross_train: 247\n",
      "y_cross_train: 247\n",
      "X_cross_train: 247\n",
      "y_cross_train: 247\n",
      "X_cross_train: 247\n",
      "y_cross_train: 247\n",
      "X_cross_train: 247\n",
      "y_cross_train: 247\n",
      "X_cross_train: 247\n",
      "y_cross_train: 247\n",
      "X_cross_train: 247\n",
      "y_cross_train: 247\n",
      "X_cross_train: 247\n",
      "y_cross_train: 247\n",
      "X_cross_train: 247\n",
      "y_cross_train: 247\n",
      "X_cross_train: 247\n",
      "y_cross_train: 247\n",
      "X_cross_train: 247\n",
      "y_cross_train: 247\n",
      "X_cross_train: 247\n",
      "y_cross_train: 247\n",
      "X_cross_train: 247\n",
      "y_cross_train: 247\n",
      "X_cross_train: 247\n",
      "y_cross_train: 247\n",
      "X_cross_train: 247\n",
      "y_cross_train: 247\n",
      "X_cross_train: 247\n",
      "y_cross_train: 247\n",
      "X_cross_train: 247\n",
      "y_cross_train: 247\n",
      "X_cross_train: 247\n",
      "y_cross_train: 247\n",
      "X_cross_train: 247\n",
      "y_cross_train: 247\n",
      "X_cross_train: 247\n",
      "y_cross_train: 247\n",
      "X_cross_train: 247\n",
      "y_cross_train: 247\n",
      "X_cross_train: 247\n",
      "y_cross_train: 247\n",
      "X_cross_train: 247\n",
      "y_cross_train: 247\n",
      "X_cross_train: 247\n",
      "y_cross_train: 247\n",
      "X_cross_train: 247\n",
      "y_cross_train: 247\n",
      "X_cross_train: 247\n",
      "y_cross_train: 247\n",
      "X_cross_train: 247\n",
      "y_cross_train: 247\n",
      "X_cross_train: 247\n",
      "y_cross_train: 247\n",
      "X_cross_train: 247\n",
      "y_cross_train: 247\n",
      "X_cross_train: 247\n",
      "y_cross_train: 247\n",
      "X_cross_train: 247\n",
      "y_cross_train: 247\n",
      "X_cross_train: 247\n",
      "y_cross_train: 247\n",
      "X_cross_train: 247\n",
      "y_cross_train: 247\n",
      "X_cross_train: 247\n",
      "y_cross_train: 247\n",
      "X_cross_train: 247\n",
      "y_cross_train: 247\n",
      "X_cross_train: 247\n",
      "y_cross_train: 247\n",
      "X_cross_train: 247\n",
      "y_cross_train: 247\n",
      "X_cross_train: 247\n",
      "y_cross_train: 247\n",
      "X_cross_train: 247\n",
      "y_cross_train: 247\n",
      "X_cross_train: 247\n",
      "y_cross_train: 247\n",
      "X_cross_train: 247\n",
      "y_cross_train: 247\n",
      "X_cross_train: 247\n",
      "y_cross_train: 247\n",
      "X_cross_train: 247\n",
      "y_cross_train: 247\n",
      "X_cross_train: 247\n",
      "y_cross_train: 247\n",
      "X_cross_train: 247\n",
      "y_cross_train: 247\n",
      "X_cross_train: 247\n",
      "y_cross_train: 247\n",
      "X_cross_train: 247\n",
      "y_cross_train: 247\n",
      "X_cross_train: 247\n",
      "y_cross_train: 247\n",
      "X_cross_train: 247\n",
      "y_cross_train: 247\n",
      "X_cross_train: 247\n",
      "y_cross_train: 247\n",
      "X_cross_train: 247\n",
      "y_cross_train: 247\n",
      "X_cross_train: 247\n",
      "y_cross_train: 247\n",
      "X_cross_train: 247\n",
      "y_cross_train: 247\n",
      "X_cross_train: 247\n",
      "y_cross_train: 247\n",
      "X_cross_train: 247\n",
      "y_cross_train: 247\n",
      "X_cross_train: 247\n",
      "y_cross_train: 247\n",
      "X_cross_train: 247\n",
      "y_cross_train: 247\n",
      "X_cross_train: 247\n",
      "y_cross_train: 247\n",
      "X_cross_train: 247\n",
      "y_cross_train: 247\n",
      "X_cross_train: 247\n",
      "y_cross_train: 247\n",
      "X_cross_train: 247\n",
      "y_cross_train: 247\n",
      "X_cross_train: 247\n",
      "y_cross_train: 247\n",
      "X_cross_train: 247\n",
      "y_cross_train: 247\n",
      "X_cross_train: 247\n",
      "y_cross_train: 247\n",
      "X_cross_train: 247\n",
      "y_cross_train: 247\n",
      "X_cross_train: 247\n",
      "y_cross_train: 247\n",
      "X_cross_train: 247\n",
      "y_cross_train: 247\n",
      "X_cross_train: 247\n",
      "y_cross_train: 247\n",
      "X_cross_train: 247\n",
      "y_cross_train: 247\n",
      "X_cross_train: 247\n",
      "y_cross_train: 247\n",
      "X_cross_train: 247\n",
      "y_cross_train: 247\n",
      "X_cross_train: 247\n",
      "y_cross_train: 247\n",
      "X_cross_train: 247\n",
      "y_cross_train: 247\n",
      "X_cross_train: 247\n",
      "y_cross_train: 247\n",
      "X_cross_train: 247\n",
      "y_cross_train: 247\n",
      "X_cross_train: 247\n",
      "y_cross_train: 247\n",
      "X_cross_train: 247\n",
      "y_cross_train: 247\n",
      "X_cross_train: 247\n",
      "y_cross_train: 247\n",
      "X_cross_train: 247\n",
      "y_cross_train: 247\n",
      "X_cross_train: 247\n",
      "y_cross_train: 247\n",
      "X_cross_train: 247\n",
      "y_cross_train: 247\n",
      "X_cross_train: 247\n",
      "y_cross_train: 247\n",
      "X_cross_train: 247\n",
      "y_cross_train: 247\n",
      "X_cross_train: 247\n",
      "y_cross_train: 247\n",
      "X_cross_train: 247\n",
      "y_cross_train: 247\n",
      "X_cross_train: 247\n",
      "y_cross_train: 247\n",
      "X_cross_train: 247\n",
      "y_cross_train: 247\n",
      "X_cross_train: 247\n",
      "y_cross_train: 247\n",
      "X_cross_train: 247\n",
      "y_cross_train: 247\n",
      "X_cross_train: 247\n",
      "y_cross_train: 247\n",
      "X_cross_train: 247\n",
      "y_cross_train: 247\n",
      "X_cross_train: 247\n",
      "y_cross_train: 247\n",
      "X_cross_train: 247\n",
      "y_cross_train: 247\n",
      "X_cross_train: 247\n",
      "y_cross_train: 247\n",
      "X_cross_train: 247\n",
      "y_cross_train: 247\n",
      "X_cross_train: 247\n",
      "y_cross_train: 247\n",
      "X_cross_train: 247\n",
      "y_cross_train: 247\n",
      "X_cross_train: 247\n",
      "y_cross_train: 247\n",
      "X_cross_train: 247\n",
      "y_cross_train: 247\n",
      "X_cross_train: 247\n",
      "y_cross_train: 247\n",
      "X_cross_train: 247\n",
      "y_cross_train: 247\n",
      "X_cross_train: 247\n",
      "y_cross_train: 247\n",
      "X_cross_train: 247\n",
      "y_cross_train: 247\n",
      "X_cross_train: 247\n",
      "y_cross_train: 247\n",
      "X_cross_train: 247\n",
      "y_cross_train: 247\n",
      "X_cross_train: 247\n",
      "y_cross_train: 247\n",
      "X_cross_train: 247\n",
      "y_cross_train: 247\n",
      "X_cross_train: 247\n",
      "y_cross_train: 247\n",
      "X_cross_train: 247\n",
      "y_cross_train: 247\n",
      "X_cross_train: 247\n",
      "y_cross_train: 247\n",
      "X_cross_train: 247\n",
      "y_cross_train: 247\n",
      "X_cross_train: 247\n",
      "y_cross_train: 247\n",
      "X_cross_train: 247\n",
      "y_cross_train: 247\n",
      "X_cross_train: 247\n",
      "y_cross_train: 247\n",
      "X_cross_train: 247\n",
      "y_cross_train: 247\n",
      "X_cross_train: 247\n",
      "y_cross_train: 247\n",
      "X_cross_train: 247\n",
      "y_cross_train: 247\n",
      "X_cross_train: 247\n",
      "y_cross_train: 247\n",
      "X_cross_train: 247\n",
      "y_cross_train: 247\n",
      "X_cross_train: 247\n",
      "y_cross_train: 247\n",
      "X_cross_train: 247\n",
      "y_cross_train: 247\n",
      "X_cross_train: 247\n",
      "y_cross_train: 247\n",
      "X_cross_train: 247\n",
      "y_cross_train: 247\n",
      "X_cross_train: 247\n",
      "y_cross_train: 247\n",
      "X_cross_train: 247\n",
      "y_cross_train: 247\n",
      "X_cross_train: 247\n",
      "y_cross_train: 247\n",
      "X_cross_train: 247\n",
      "y_cross_train: 247\n",
      "X_cross_train: 247\n",
      "y_cross_train: 247\n",
      "X_cross_train: 247\n",
      "y_cross_train: 247\n",
      "X_cross_train: 247\n",
      "y_cross_train: 247\n",
      "X_cross_train: 247\n",
      "y_cross_train: 247\n",
      "X_cross_train: 247\n",
      "y_cross_train: 247\n",
      "X_cross_train: 247\n",
      "y_cross_train: 247\n",
      "X_cross_train: 247\n",
      "y_cross_train: 247\n",
      "X_cross_train: 247\n",
      "y_cross_train: 247\n",
      "X_cross_train: 247\n",
      "y_cross_train: 247\n",
      "X_cross_train: 247\n",
      "y_cross_train: 247\n",
      "X_cross_train: 247\n",
      "y_cross_train: 247\n",
      "X_cross_train: 247\n",
      "y_cross_train: 247\n",
      "X_cross_train: 247\n",
      "y_cross_train: 247\n",
      "X_cross_train: 247\n",
      "y_cross_train: 247\n",
      "X_cross_train: 247\n",
      "y_cross_train: 247\n",
      "X_cross_train: 247\n",
      "y_cross_train: 247\n",
      "X_cross_train: 247\n",
      "y_cross_train: 247\n",
      "X_cross_train: 247\n",
      "y_cross_train: 247\n",
      "X_cross_train: 247\n",
      "y_cross_train: 247\n",
      "X_cross_train: 247\n",
      "y_cross_train: 247\n",
      "X_cross_train: 247\n",
      "y_cross_train: 247\n",
      "X_cross_train: 247\n",
      "y_cross_train: 247\n",
      "X_cross_train: 247\n",
      "y_cross_train: 247\n",
      "X_cross_train: 247\n",
      "y_cross_train: 247\n",
      "X_cross_train: 247\n",
      "y_cross_train: 247\n",
      "X_cross_train: 247\n",
      "y_cross_train: 247\n",
      "X_cross_train: 247\n",
      "y_cross_train: 247\n",
      "X_cross_train: 247\n",
      "y_cross_train: 247\n",
      "X_cross_train: 247\n",
      "y_cross_train: 247\n",
      "X_cross_train: 247\n",
      "y_cross_train: 247\n",
      "X_cross_train: 247\n",
      "y_cross_train: 247\n",
      "X_cross_train: 247\n",
      "y_cross_train: 247\n",
      "X_cross_train: 247\n",
      "y_cross_train: 247\n",
      "X_cross_train: 247\n",
      "y_cross_train: 247\n",
      "X_cross_train: 247\n",
      "y_cross_train: 247\n",
      "X_cross_train: 247\n",
      "y_cross_train: 247\n",
      "X_cross_train: 247\n",
      "y_cross_train: 247\n",
      "X_cross_train: 247\n",
      "y_cross_train: 247\n",
      "X_cross_train: 247\n",
      "y_cross_train: 247\n",
      "X_cross_train: 247\n",
      "y_cross_train: 247\n",
      "X_cross_train: 247\n",
      "y_cross_train: 247\n",
      "X_cross_train: 247\n",
      "y_cross_train: 247\n",
      "X_cross_train: 247\n",
      "y_cross_train: 247\n",
      "X_cross_train: 247\n",
      "y_cross_train: 247\n",
      "X_cross_train: 247\n",
      "y_cross_train: 247\n",
      "X_cross_train: 247\n",
      "y_cross_train: 247\n",
      "X_cross_train: 247\n",
      "y_cross_train: 247\n",
      "X_cross_train: 247\n",
      "y_cross_train: 247\n",
      "X_cross_train: 247\n",
      "y_cross_train: 247\n",
      "X_cross_train: 247\n",
      "y_cross_train: 247\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(500):\n",
    "    '''\n",
    "    shuffle the features' indices\n",
    "    with replacement or no replacement\n",
    "    with replacement means just randomize\n",
    "    with no replacement means 0:50, 51:100, 101:150, ......300:323\n",
    "    shuffle your index\n",
    "    '''\n",
    "    X_cross_train = X_cross_train[perm]\n",
    "    y_cross_train = y_cross_train[perm]\n",
    "\n",
    "    print(f'X_cross_train: {len(X_cross_train)}')\n",
    "    print(f'y_cross_train: {len(y_cross_train)}')\n",
    "\n",
    "    # if stochastic batch:\n",
    "    for i in range(X_cross_train.shape[0]):\n",
    "        break\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "247"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_cross_train.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_method_train: (1, 11)\n",
      "X_method_train: <class 'numpy.ndarray'>\n",
      "y_method_train: 321.0\n",
      "y_cross_train: <class 'numpy.float64'>\n"
     ]
    }
   ],
   "source": [
    "# stochastic method\n",
    "for i in range(X_cross_train.shape[0]):\n",
    "\n",
    "    X_method_train = X_cross_train[i].reshape(1,-1)\n",
    "    \n",
    "    print(f'X_method_train: {X_method_train.shape}')\n",
    "    print(f'X_method_train: {type(X_method_train)}')\n",
    "\n",
    "    # y_method_train = np.array([y_cross_train[i]])  \n",
    "\n",
    "\n",
    "    y_method_train = y_cross_train[i]\n",
    "\n",
    "    print(f'y_method_train: {y_method_train}')\n",
    "    print(f'y_cross_train: {type(y_method_train)}')\n",
    "\n",
    "    # _train -> Indicates that the name is meant for internal use only\n",
    "    # train_loss = _train(X_method_train, y_method_train)\n",
    "    break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_cross_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_method_train: (50, 11)\n",
      "X_method_train: 50\n",
      "y_method_train: (50,)\n",
      "y_method_train: 50\n"
     ]
    }
   ],
   "source": [
    "# mini batch method\n",
    "batch_size = 50\n",
    "for i in range(0, X_cross_train.shape[0]):\n",
    "    '''\n",
    "    0: 50: 247 \n",
    "    #batch_idx = 0, 50, 100, 150\n",
    "    '''\n",
    "    X_method_train = X_cross_train[i:i+batch_size,:]\n",
    "\n",
    "    print(f'X_method_train: {X_method_train.shape}')\n",
    "    print(f'X_method_train: {len(X_method_train)}')\n",
    "\n",
    "    y_method_train = y_cross_train[i:i+batch_size]\n",
    "\n",
    "    print(f'y_method_train: {y_method_train.shape}')\n",
    "    print(f'y_method_train: {len(y_method_train)}')\n",
    "\n",
    "    # train_loss = _train(X_method_train, y_method_train)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_cross_train.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# full batch method\n",
    "X_method_train = X_cross_train\n",
    "y_method_train = y_cross_train\n",
    "# train_loss = self._train(X_method_train, y_method_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "np.allclose\n",
    "Returns True if two arrays are element-wise equal within a tolerance.\n",
    "\n",
    "'''\n",
    "\n",
    "a = [2,3,4,5,6,7,9,9,9]\n",
    "b = [3,5,6,7,7,8,9,9,9]\n",
    "\n",
    "if np.allclose(a,b):\n",
    "    print('stopped')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Simulated validation losses over epochs\n",
    "validation_losses = [0.5, 0.4, 0.35, 0.34, 0.34, 0.34]\n",
    "\n",
    "# Initialize previous loss to infinity\n",
    "prev_loss = np.infty\n",
    "\n",
    "for epoch, current_loss in enumerate(validation_losses):\n",
    "    print(f\"Epoch {epoch}, validation loss: {current_loss}\")\n",
    "    \n",
    "    # If the current loss is almost equal to the previous loss, break the loop (early stopping)\n",
    "    if np.allclose(current_loss, prev_loss):\n",
    "        print(f\"Early stopping at epoch {epoch} because loss did not change significantly.\")\n",
    "        break\n",
    "    \n",
    "    # Update previous loss\n",
    "    prev_loss = current_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#helper function for looping classnames\n",
    "import sys\n",
    "\n",
    "def str_to_class(classname):\n",
    "    return getattr(sys.modules[__name__], classname)\n",
    "\n",
    "''' \n",
    "sys.modules[__name__]: Gets the current module.\n",
    "getattr(...): Looks up the attribute (class) with the name provided in the string.\n",
    "Usage: If you have a class named Example defined in the module, calling str_to_class(\"Example\") returns the Example class object.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "class A:\n",
    "    def __init__(self):\n",
    "        self.value = \"Instance of Class A\"\n",
    "\n",
    "    def show(self):\n",
    "        print(self.value)\n",
    "\n",
    "class B:\n",
    "    def __init__(self):\n",
    "        self.value = \"Instance of Class B\"\n",
    "\n",
    "    def show(self):\n",
    "        print(self.value)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert string to class\n",
    "A_class = str_to_class(\"A\")\n",
    "B_class = str_to_class(\"B\")\n",
    "\n",
    "# Create instances\n",
    "a_instance = A_class()\n",
    "b_instance = B_class()\n",
    "\n",
    "# Use the instances\n",
    "a_instance.show()  # Outputs: Instance of Class A\n",
    "b_instance.show()  # Outputs: Instance of Class B\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lasso\n",
    "\n",
    "class Lasso:\n",
    "    def __int__(self, l): # lambda is a constant parameter\n",
    "        self.l = l\n",
    "    \n",
    "    def __call__(self, theta): # allows us to call class as method # theta is a dynamic input\n",
    "        return self.l * np.sum(np.abs(theta))\n",
    "\n",
    "    def derivation(self, theta):\n",
    "        return self.l * np.sign(theta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ridge\n",
    "# Standalone class\n",
    "\n",
    "\n",
    "class Ridge:\n",
    "    def __int__(self, l): # lambda is a constant parameter\n",
    "        self.l = l\n",
    "    \n",
    "    def __call__(self, theta): # allows us to call class as method # theta is a dynamic input\n",
    "        return self.l * np.sum(np.square(theta))\n",
    "\n",
    "    def derivation(self, theta):\n",
    "        return self.l * 2 * theta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "J(\\theta) = \\frac{1}{2}\\sum_{i=1}^m(h_\\theta(x^{(i)}) - y^{(i)})^2 + \\lambda(\\lambda \\sum_{j=1}^n |\\theta_j| + (1 - \\lambda) \\sum_{k=1}^n \\theta_k^2)\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ElasticNet\n",
    "\n",
    "class ElasticNet:\n",
    "    def __int__(self, l, l_ratio): # lambda is a constant parameter\n",
    "        self.l       = l\n",
    "        self.l_ratio = l_ratio\n",
    "    \n",
    "    def __call__(self, theta): # allows us to call class as method # theta is a dynamic input\n",
    "        l1 =  self.l * (1 - self.l_ratio) * np.sum(np.abs(theta))\n",
    "        l2 =  self.l * self.l_ratio * np.sum(np.square(theta))\n",
    "        return (l1 + l2)\n",
    "\n",
    "    def derivation(self, theta):\n",
    "        l1 = self.l * self.l_ratio * np.sign(theta)\n",
    "        l2 = 2 * self.l * (1 - self.l_ratio) * theta\n",
    "        return (l1 + l2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' \n",
    "Ridge Regression:\n",
    "Imagine you have many ingredients (predictors) in a recipe and you believe every one of them adds something, even if they are a bit similar. Ridge doesn't remove any ingredients; it just uses them in smaller, balanced amounts. This helps keep the overall mix stable, especially when some ingredients might be too similar.\n",
    "\n",
    "Lasso Regression:\n",
    "Think of Lasso like a strict chef who wants to simplify the recipe. The chef decides that only a few key ingredients really matter and completely leaves out the rest. Lasso automatically sets some of the coefficients to zero, effectively removing the less important predictors from the model.\n",
    "\n",
    "Elastic Net:\n",
    "Elastic Net is like a chef who uses a combination of both approaches. It sometimes reduces the influence of ingredients (like Ridge) and sometimes completely drops them (like Lasso). This approach is useful when you have many ingredients, and some are very similar—you want to keep the good parts of both techniques.\n",
    "\n",
    "In short, Ridge is best when you want to keep all predictors but control their impact, Lasso is best when you want to simplify by keeping only the important ones, and Elastic Net is a balanced mix that handles situations where predictors are numerous and possibly similar.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# inhertis linear regression and has separate classes for each of this regularization algorithm\n",
    "\n",
    "class LassoRegression(LinarRegression):\n",
    "    def __init__(self, method, lr, l):\n",
    "        self.regularization = Lasso(l)\n",
    "        super().__init__(self.regularization, lr, method)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parent class\n",
    "\n",
    "class Animal:\n",
    "    def __init__(self, name, breed):\n",
    "        self.name = name\n",
    "        self.breed = breed\n",
    "\n",
    "    def speak(self):\n",
    "        print(\"This animal makes sound\")\n",
    "\n",
    "# Child class\n",
    "\n",
    "class Dog(Animal):\n",
    "    def __init__(self, name, breed):\n",
    "        super().__init__(name, breed)\n",
    "    def speak(self):\n",
    "        print(f'{self.name} says: Woof!!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_dog = Dog('buddy', 'gr')\n",
    "\n",
    "my_dog.name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "np.sign(-200000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# signature = mlflow.models.infer_signature(X_train, model.predict(X_train))\n",
    "\n",
    "''' \n",
    "Model signature defines schema of model input and output.\n",
    "\n",
    "Log a scikit-learn model as an MLflow artifact for the current run.\n",
    "\n",
    " To manually infer a model signature, call infer_signature() on datasets with valid model inputs, \n",
    " such as a training dataset with the target column omitted, and valid model outputs, like model predictions made on the training dataset\n",
    "\n",
    "'''\n",
    "# mlflow.sklearn.log_model(model, artifact_path='model', signature=signature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "m = 10  # Number of input neurons (fan-in)\n",
    "# Calculate the range for Xavier initialization\n",
    "lower, upper = -(1.0 / np.sqrt(m)), (1.0 / np.sqrt(m))\n",
    "\n",
    "print(lower, upper)  # Print calculated range"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate 1000 random numbers in [0, 1)\n",
    "numbers = np.random.random(1000)\n",
    "\n",
    "# Scale to the range [lower, upper]\n",
    "scaled = lower + numbers * (upper - lower)\n",
    "\n",
    "print(scaled)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml-dl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
